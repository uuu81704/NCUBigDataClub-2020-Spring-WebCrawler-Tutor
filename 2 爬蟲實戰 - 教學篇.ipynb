{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大數據研究社 1450の網頁爬蟲實戰\n",
    "\n",
    "學會爬蟲就可以加入網軍了嗎? 可以領1450嗎? 既然大家都被標題騙來了，就看下去吧ヾ(●゜▽゜●)\n",
    "\n",
    "## 爬蟲是什麼\n",
    "網路爬蟲是一個可以自動化抓取網頁內容的程式。\n",
    "\n",
    "相信大家多少都遇過需要抓取網頁資訊的時候，也許是因為要做報告、或是出於興趣想研究，需要相關參考資料。最簡單的方法就是一筆一筆複製，然後貼到excel或是文字編輯器儲存，再做後續的分析。\n",
    "\n",
    "如果只有幾十筆還好，那假如有上百筆、上千筆怎麼辦？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景知識\n",
    "\n",
    "網頁爬蟲是利用程式模仿瀏覽器，透過HTTP(S)請求向伺服器取得**網頁原始碼**，再從擷取到網頁原始碼蒐集**想要的資訊**。網頁爬蟲可以很簡單，難度取決於我們想爬的網站的設計和資料得詳細程度。\n",
    "\n",
    "網頁爬蟲可以分成2步驟\n",
    "- 第一步是取得網頁原始碼(HTML)\n",
    "- 第二步是解析HTML原始碼，找出我們要的資訊\n",
    "\n",
    "![](https://drive.google.com/uc?export=download&id=1UiDUXriZVo83ePdiSQTLS4QWnCN_YsVu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scontent.frmq2-2.fna.fbcdn.net/v/t1.0-9/p960x960/41313754_306610563225314_4813346634328965120_o.jpg?_nc_cat=109&_nc_oc=AQlhNysrF9hrCiLlCwn2iOtHOZu1J8e051CaXd13EC5L_YyewojFAdO95M4LJyLz1ic&_nc_ht=scontent.frmq2-2.fna&_nc_tp=6&oh=346011528de001b6dd81937095240e45&oe=5EBD24E1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套件介紹\n",
    "\n",
    "* Requests [官方文件](https://requests.readthedocs.io/en/master/user/quickstart/)\n",
    "* BeautifulSoup [官方文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)\n",
    "\n",
    "### 1. Requests 套件\n",
    "Requests 是一個簡潔又優雅的 Python HTTP 套件，可以一行搞定 HTTP 連線。\n",
    "\n",
    "安裝指令: `$ pip install requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://pythonscraping.com/pages/warandpeace.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們成功取得到了對方伺服器的回應(Response)，代號是200。\n",
    "接著用一個變數`res`來接收Response物件，取出HTML網頁原始碼吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://pythonscraping.com/pages/warandpeace.html\")\n",
    "# print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BeautifulSoup 套件\n",
    "\n",
    "Beautiful Soup [官方文件](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/) 是一個可以從HTML或XML文件中提取數據的Python庫。它能夠通過你喜歡的轉換器實現慣用的文檔導航,查找,修改文檔的方式。Beautiful Soup會幫你節省數小時甚至數天的工作時間。典故來自 Alice in WonderLand 裏面一首同名的詩，由假海龜 (Mock Turtle) 所唱，影射英國料理假海龜湯...\n",
    "\n",
    "安裝指令: `$ pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"http://pythonscraping.com/pages/warandpeace.html\")\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好的! 成功使用bs4解析範例網站，我們習慣把解析出來的物件存入名稱為`soup`的變數，試試把所有的**人物名字**都給記錄下來吧。\n",
    "\n",
    "### bs4套件提供幾個函式來選取標籤\n",
    "\n",
    "- `findAll()` 是找出**所有**符合條件的標籤\n",
    "\n",
    "- `find()` 是找到**第一筆**符合條件的資料\n",
    "\n",
    "- `select()` 是找出**所有**符合條件的標籤 Always use css selectors when chaining tags or using `tag.classname`. [Beautifulsoup : Is there a difference between .find() and .select()](https://stackoverflow.com/questions/38028384/beautifulsoup-is-there-a-difference-between-find-and-select-python-3-x)\n",
    "\n",
    "- `select_one()` 是找到**第一筆**符合條件的資料\n",
    "\n",
    "下面這2行會得到一樣的結果: 選出class名稱是green的所有標籤\n",
    "```python\n",
    "soup.findAll(\"span\", { \"class\": \"green\"})\n",
    "soup.select('span.green')\n",
    "```\n",
    "\n",
    "### 取出標籤中的文字\n",
    "\n",
    "- `.text` 是取出tag和它底下的其他tags**所有**的文字\n",
    "\n",
    "- `.string` 是取出**這一個tag**裡面的文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "name_list = soup.findAll(\"span\", { \"class\": \"green\"})\n",
    "for n in name_list:\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，最後提醒一下爬蟲是個走後門的方法，爬蟲程式既脆弱又繁複，我們來看一下爬蟲會面臨的問題:\n",
    "- 對方網站框架太新潮，根本無法爬蟲。例如 react.js, Gmail, FB, Twitter 等\n",
    "- 對方網頁維護/改版，爬蟲程式也要跟著更新\n",
    "\n",
    "所以重要的資料有 api 可以使用的話，付點費購買還是值得的 ┐(´д`)┌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
